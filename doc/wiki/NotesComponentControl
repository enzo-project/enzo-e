([NotesComponentArray Array] < [NotesComponents index] > [NotesComponentDisk Disk])

= `Control` Component =

[[TOC]]

The `Control` component is the main scheduler in Cello, and orchestrates the distributed computations to advance the simulation in time, as well as simulation initialization and finalization.  Currently we are evaluating the use of [http://charm.cs.uiuc.edu CHARM++] in the `Control` component.

   * execute the simulation
     * create tasks
     * schedule tasks
     * trap errors
     * handle errors

== `Control` Supporting Classes ==

 * [NotesComponentTask Task]: A `Task`, which is part of the `Parallel` component, is the combination of a sequence of `Method`'s (algorithms) and a `FieldBlock` (data).  Parallel distribution and load-balancing of `Task`s is controlled by the `Parallel` component, whereas scheduling of inter-`Task` communication and scheduling of `Task` methods is controlled by the `Control` component.  Since each `FieldBlock` corresponds to a serial thread, a `Task` is analagous to a "Chare" object in CHARM++.

== `Control` Functions ==

  || `Control()` || || 
  ||  `~Control()` || || 
  || `create_tasks()` ||  ''Create one or more `Task`s'' || 
  || `run_tasks()` ||  ''Execute one or more `Task`s'' || 
  || `delete_tasks()` || ''Delete one or more `Task`s'' || 
  || `push_tasks()` || ''Push one or more `Task`s on a queue'' ||
  || `pop_tasks()` || ''Pop one or more `Task`s from a queue'' ||

== Patch scheduling ==

Compared to Enzo, the `Control` component does not loop through each level individually;  instead, it collapses the dependency tree to its theoretical shortest path, which is that of the finest level timesteps.  E.g. consider a 1-D problem with two levels of refinement near the center.  The parallel steps for Enzo versus Cello are shown below, with timesteps moving downward:

{{{
      Enzo: parallel levels               Cello: parallel level intervals
   +-------+---+-+-+---+-------+          +-------+---+-+-+---+-------+
   |   X   |   | | |   |   X   | 2        |   X   | X |X|X| X |   X   | 6
   +-------+---+-+-+---+-------+          +-------+---+-+-+---+-------+
   |       | X | | | X |       | 2        |       |   |X|X|   |       | 2
   +-------+---+-+-+---+-------+          +-------+---+-+-+---+-------+
   |       |   |X|X|   |       | 2        |       | X |X|X| X |       | 4
   +-------+---+-+-+---+-------+          +-------+---+-+-+---+-------+
   |       |   |X|X|   |       | 2        |       |   |X|X|   |       | 2
   +-------+---+-+-+---+-------+
   |       | X | | | X |       | 2
   +-------+---+-+-+---+-------+
   |       |   |X|X|   |       | 2
   +-------+---+-+-+---+-------+
   |       |   |X|X|   |       | 2
   
}}}

The advantage of the Cello approach is that, assuming sufficient parallelism, the finest level can be advancing continuously.  The disadvantage is that the load is inherently less balanced, since in Enzo parallel tasks are disjoint (individual levels), compared to successively inclusive (ranging between "just the finest level" to "all levels").  In the example, Enzo is perfectly balanced with 2 tasks active at each step. 
The Cello approach is never worse: with no parallelism it is the same as the Enzo approach, and with infinite parallelism it's twice as fast for this example. However, with a "practical" amount of parallelism, they will still be close, since most of the work is at the finest timestep, and that's also the limiting factor in parallelization.

Alternatively, one could run the entire simulation at the finest timestep.  If sufficient parallelism is available, a simulation would still run about as fast, and would make much higher utilization of the parallel hardware (load balancing would be much better).

{{{
         Uniform timestep
   +-------+---+-+-+---+-------+
 6 |   X   | X |X|X| X |   X   |
   +-------+---+-+-+---+-------+
 6 |   X   | X |X|X| X |   X   |
   +-------+---+-+-+---+-------+
 6 |   X   | X |X|X| X |   X   | 
   +-------+---+-+-+---+-------+
 6 |   X   | X |X|X| X |   X   |
   +-------+---+-+-+---+-------+

}}}

As an alternative approach, it may be possible to take advantage of the load imbalance of the "Cello approach", for example by scheduling I/O tasks on other nodes, or even interleaving/pipelining two independent simulations(!)  Even though independent simulations are "embarrasingly parallel" and could be implemented as such, this interleaved approach may potentially greatly increase the efficiency of using massively parallel architectures.

{{{
            Simulation 1
   +-------+---+-+-+---+-------+
 6 |   X   | X |X|X| X |   X   |         Simulation 2
   +-------+---+-+-+---+-------+ +-------+---+-+-+---+-------+
 8 |       |   |X|X|   |       | |   O   | O |O|O| O |   O   |
   +-------+---+-+-+---+-------+ +-------+---+-+-+---+-------+
 6 |       | X |X|X| X |       | |       |   |O|O|   |       |
   +-------+---+-+-+---+-------+ +-------+---+-+-+---+-------+
 6 |       |   |X|X|   |       | |       | O |O|O| O |       |
   +-------+---+-+-+---+-------+ +-------+---+-+-+---+-------+
 8 |   X   | X |X|X| X |   X   | |       |   |O|O|   |       |
  
}}}

If task scheduling is dynamic, then it may be possible to improve efficiency by ensuring that finer level tasks that are adjacent to coarse level tasks are performed first, so that coarse level tasks can begin earlier, and non-adjacent fine level tasks can run in parallel with coarser level tasks.  Since a bulk of the work is at the fine level, the main advantage of this would be more to reduce overhead time between steps, which may be significant for large numbers of processors.  The advantage of running multiple levels at once may be less advantageous, since the workload at the finest level is about the same as all other levels combined (for the example).  However, it may be used to hide latency from communication, and it may help to free up processor groups faster to improve multitasking (say) I/O with computation.

As an oversimplified summary:

 * The speed of Cello is theoretically faster than Enzo, by up to a factor of two
 * The parallel efficiency of Enzo is theoretically better than Cello
 * Uniform time-stepping is preferred when sufficient parallelism is available
 * Multiple simulations may be "interleaved" to gain higher parallel efficiency than single simulations
 * Intelligent dynamic scheduling might improve efficiency
