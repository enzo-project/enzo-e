%0       1         2         3         4         5         6         7         8
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%=======================================================================
\documentclass{book}
%=======================================================================

\include{include}

%=======================================================================

\begin{document}

%=======================================================================
\TITLE{Software Requirements Specifications}{James Bordner}{\textbf{v0.1}}
%=======================================================================

%=======================================================================
\chapter{Introduction} \label{s:intro}
%=======================================================================

% Overview
   \cello\ is intended to be a high-performance multi-resolution
   astrophysics application, capable of running efficiently on
   large-scale (\~{\ }$O(10^6)$ cores) parallel supercomputers.
   Physics capabilities will include hydrodynamics, representations of
   both dark matter (via particles) and baryonic matter (via fields),
   cosmological expansion, self-gravity, radiative cooling,
   multispecies chemistry, magnetohydrodynamics, and radiation
   transfer.

% AMR
   Computations will be performed at multiple spacial and temporal
   resolutions using adaptive mesh refinement techniques (AMR).
   Resolution of multiple grid patches (structured AMR) or individual
   zones (cell-based AMR) will be dynamically controlled using a
   user-specified set of refinement criteria.  This will permit the
   physics modules to capture the full range of scales of interest,
   but without over-refining other regions and leading to excessive
   computation and memory storage.  Algorithms and data-structures will
   be chosen to maximize scalability in both computation and memory
   usage.  Multiple implementations will be made available to provide
   flexibility in optimizing for the target architecture and problem.
   Characteristics of the AMR hierarchies, e.g.~grid patch
   characteristics (size, size quantization, shape, etc.) will be
   flexible to allow for optimization of degree of over-refinement,
   computational task size, and task computation efficiency.  SAMR and
   CAMR methods will be available either separately or together.
   
% Parallelization: horizontal data movement optimization

   Concurrency will be modularized to improve flexibility in choosing
   the best method of parallelization for a given problem on a given
   parallel platform.  MPI, MPI2, OpenMP, UPC, and will include hybrid
   schemes such as MPI + OpenMP, or MPI + UPC, where possible and if
   appropriate.  Other schemes such as using shared memory array
   libraries or pthreads may be considered.  Parallelization methods
   will be primarily data parallelism, though support for
   collaberative (functional) parallelism and pipelining will be
   considered.

%  load balancing

   Parallel tasks will be load balanced using hierarchical dynamic
   load balancing algorithms.  Load balancing schemes will use
   performance information gathered by the running application to make
   load balancing decisions, and will allow flexibility in optimizing
   the parallel distribution of computation, memory storage, or a
   combination of both.  Combining flexible hierarchical
   parallelization schemes with hierarchical load balancing
   algorithms, together with scalable algorithms and efficient AMR
   data-structures, are expected to lead to high parallel efficiency
   and scalability.

% Field data layout: vertical data movement optimization

   The properties of individual grid patches, and details of how
   scalar and vector fields are stored on grid patches, will be
   flexible to allow optimization of the use of deep memory/cache
   hierarchies.  This includes hierarchical blocking of grid data to 
   maximize reuse of data in caches, padding of arrays to reduce
   cache thrashing effects for low-associativity caches, and interleaving
   vector field data or select scalar fields to improve data locality.
   These capabilities, together with efficient algorithms and 
   implementation of computations, are expected to lead to high
   single-thread computational efficiency and data movement through
   memory/cache hierarchies.

%  Performance monitoring

   Comprehensive performance-related measurements will be continuously
   collected for simulations.  By ``comprehensive'' we mean parallel
   communication amount, memory and computational load balance
   efficiency, memory usage, memory reference and floating point
   operation counts, and disk usage.

% Input files (small I/O)

   Problems will be specified using an input parameter file.  The set
   of problem parameters available will be sufficient to set up all
   known test problems in the literature that can be run given the
   available physics capabilities.  Parameters will include deep
   control of the internal physical quantities, units, physics
   algorithms, and data-structures.

   Information regarding the progress of the simulation will be 
   output to the user in a format or formats that are both easy
   to read, and easy to post-process using external programs or
   utilities.
   
%  I/O (big I/O)

   Several options for how data fields and particles are read and
   written to disk will be implemented, to improve flexibility and
   allow optimization of I/O for a given problem on a given parallel
   platform.  Parallel HDF5 will be used to optimize efficiency and
   portability.  Multiple data layouts within and between files will
   be implemented to allow optimization of I/O to a particular problem
   and file system.

%  I/O dumps, inline analysis, visualization

   Support will be available for writing the whole or part of some or
   all data fields and particles at specified times.  I/O dumps will
   include targeted support for both check-pointing (for automatic use
   by the software error recovery) and data dumps (for subsequent user
   use).  
   
%  Error recovery

   Support will be included for error prevention, identification and
   recovery.  This will include fault-tolerance methods to handle
   possible hardware faults, and self-monitoring of fields, particles,
   and other variables against physics quantity invariants
   (e.g.~positive densities, mass conservation, etc.)

% Self-tuning

   Support will be included for tuning of various components to the
   running application and hardware platform.  Tuning will be either
   self-tuning by the running application as it monitors its
   performance, or by the user monitoring the performance of a running
   simulation.  Adaptivity will be available for physics, physics
   algorithms, resolution ranges, AMR data-structures and algorithms,
   field and particles data-structures, parallelism.

%=======================================================================
\chapter{Input} \label{s:inputs}
%=======================================================================

   This chapter specifies the input format for \cello.  Input is from
   a user file containing parameters.  The input file may optionally
   include other input files.

   Parameters are organized into functional groups, which are described
   in more detail in following sections.

\begin{description}

 \item [Domain (\S\ref{s:domain}): ] Specify properties of the domain, such as extents.

 \item [Boundary conditions (\S\ref{s:boundary}): ] Including periodic, in- and out-flow,
 specified, and dynamic, each on separate faces or portions of faces,
 and different for different fields.

 \item [Regions (\S\ref{s:region}): ] Specify partitions of the domain into regions.
 Each region contains different materials with different properties.
 Example partitions may be half-planes, spheres, boxes, or specified
 using a file containing a zone bit mask.  Default is region 0, first
 region is region 1, etc.  Use solid modeling representations?

 \item [Materials (\S\ref{s:material}): ] Material properties, such as gas constant, etc.
 [Merge with Fields?]

 \item [Fields (\S\ref{s:field}): ] Scalar and vector fields for each material, such as
 density, energy, velocity, etc.  [Merge with Materials?]  Specify
 values, or input from files.

 \item [Units (\S\ref{s:units}): ] Specify units and optional scalings for individual
 fields.  [Merge units with Simulation?] [Merge scaling with Fields?] 
 [Dynamic scaling, e.g.~to keep average of all fields near one.]

 \item [Physics (\S\ref{s:physics}): ] Specify physics modules and physics parameters,
 including hydrodynamics, self-gravity, gravitational constant,
 imposed gravity, chemistry, cosmological expansion, star formation,
 etc.  Physics is in the problem domain.

 \item [Algorithms (\S\ref{s:algorithm}): ] Specify the algorithms and algorithm parameters
 to use for each physics component.  Each physics component has a
 default; some components may have only one available
 (e.g.~cosmological expansion).  Algorithms is in the solution domain.

 \item [Data structures (\S\ref{s:data}): ] Specify data structures and data structure
 parameters, such as number of mesh levels, AMR method, grid patch
 properties, rebuild algorithm, dynamic load balancing, refinement
 criteria, grid patch sub-block size for cache reuse, etc.

 \item [Simulation (\S\ref{s:simulation}): ] Given Physics, Algorithms, and Data structures,
 specify the top-level sequencing and properties of the simulation.
 For example, ordering of physics modules, whether to do hierarchical
 time-stepping, up to what level, whether to sub-cycle some physics,
 etc. [Is this a useful category?]  Also include things like floors
 and limits(?), and IO dumps

 \item [Parallelism (\S\ref{s:parallel}): ] Specify parallelism type and parameters.  For
 example, non-blocking MPI, MPI-2, hybrid MPI/UPC, performance-related
 parameters such as buffer size, etc.

 \item [Error recovery (\S\ref{s:error}): ] Fault tolerance and adaptivity parameters

 \item [Performance monitoring (\S\ref{s:performance}): ] Performance monitoring and
 optimization(?) parameters
\end{description}


%=======================================================================
\section{Domain parameters} \label{s:domain}
%=======================================================================

Specify properties of the domain.

Domains are boxes aligned with the computational coordinate system, and are uniquely
determined by the spacial dimension, and the lowest and highest points in the domain.

%-----------------------------------------------------------------------
\subsection{Parameters}
%-----------------------------------------------------------------------

\begin{enumerate}
\item \code{Domain:dimension} $([3])$
\item \code{Domain:lower\_point} \textit{Position} ([0])
\item \code{Domain:upper\_point} \textit{Position} ([1])
\end{enumerate}

Positions are described as $n$-tuples delimited by square brackets.
The default dimension is $3$, and the default extents are from
$[0,0,0]$ (or $[0,0]$ / $[0]$) to $[1,1,1]$ (or $[1,1]$ / $[1]$).
Lower and upper points are given in units given by spacial Units
parameters, described in \S\ref{s:units}.

%-----------------------------------------------------------------------
\subsection{Examples}
%-----------------------------------------------------------------------

\begin{verbatim}

   // Specify the domain

   Domain:dimension      3
   Domain:lower_point    [-3e9,-3e9,-3e9]
   Domain:upper_point    [ 3e9, 3e9, 3e9]

\end{verbatim}

%-----------------------------------------------------------------------
\subsection{Restrictions}
%-----------------------------------------------------------------------

\begin{enumerate}
\item Each coordinate of the lower point must be strictly greater than the corresponding coordinate of the upper point.
\item The number of coordinates in both lower and upper points must equal the dimension.
\item If the dimension is not specified, a value of $3$ is assumed.
\item The dimension must be 1, 2, or 3.
\end{enumerate}

%=======================================================================
\section{Boundary condition parameters} \label{s:boundary}
%=======================================================================

Different types of boundary conditions are supported, including
periodic, in- and out-flow, specified, and dynamic.  Different
boundary conditions can be specified for the entire domain,
on separate faces, on subregions of faces, or on specific zones.
Different boundary conditions can be specified for different fields.

%-----------------------------------------------------------------------
\subsection{Parameters}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\subsection{Examples}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\subsection{Assumptions}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\subsection{Restrictions}
%-----------------------------------------------------------------------


%=======================================================================
\section{Region parameters} \label{s:region}
%=======================================================================

Specify partitions of the domain into regions.  Each region contains
different materials with different properties.  Example partitions may
be half-planes, spheres, boxes, or specified using a file containing a
zone bit mask.  Default is region 0, first region is region 1, etc.
Use solid modeling representations?

%=======================================================================
\section{Material parameters} \label{s:material}
%=======================================================================

 Material properties, such as gas constant, etc.
 [Merge with Fields?]

%=======================================================================
\section{Field parameters} \label{s:field}
%=======================================================================

Scalar and vector fields for each material, such as
 density, energy, velocity, etc.  [Merge with Materials?]  Specify
 values, or input from files.

%=======================================================================
\section{Units parameters} \label{s:units}
%=======================================================================

 Specify units and optional scalings for individual
 fields.  [Merge units with Simulation?] [Merge scaling with Fields?] 
 [Dynamic scaling, e.g.~to keep average of all fields near one.]

%=======================================================================
\section{Physics parameters} \label{s:physics}
%=======================================================================

Specify physics modules and physics parameters, including
hydrodynamics, self-gravity, gravitational constant, imposed gravity,
chemistry, cosmological expansion, star formation, etc.  Physics is in
the problem domain.

Specify physics components

\begin{itemize}
\item hydrodynamics
\item  cosmological expansion
\item self-gravity
\end{itemize}
%=======================================================================
\section{Algorithm parameters} \label{s:algorithm}
%=======================================================================

 Specify the algorithms and algorithm parameters
 to use for each physics component.  Each physics component has a
 default; some components may have only one available
 (e.g.~cosmological expansion).  Algorithms is in the solution domain.

\begin{itemize}
\item PPM hydro (dual-energy, etc.)
\item gravity solver (FAC, smoother, levels, etc.)
\end{itemize}
%=======================================================================
\section{Data structure parameters} \label{s:data}
%=======================================================================

Specify data structures and data structure parameters, such as number
of mesh levels, AMR method, grid patch properties, rebuild algorithm,
dynamic load balancing, refinement criteria, grid patch sub-block size
for cache reuse, etc.

Field storage (blocked, padded, interleved)

\begin{itemize}
\item PatchAMR (levels, grid size or count, rebuild method, distribution)
\end{itemize}

%=======================================================================
\section{Simulation parameters} \label{s:simulation}
%=======================================================================

Given Physics, Algorithms, and Data structures, specify the top-level
sequencing and properties of the simulation.  For example, ordering of
physics modules, whether to do hierarchical time-stepping, up to what
level, whether to sub-cycle some physics, etc. [Is this a useful
category?]  Also include things like floors and limits(?), and IO
dumps

Global simulation control.

Output types and parameters

\begin{itemize}
\item checkpoint (dump all)
\item output (specific fields)
\item movies (type and rate)
\item analysis (type of analysis, rate)
\item level of output (files for timestep, time, etc.)
\end{itemize}

%=======================================================================
\section{Parallelism parameters} \label{s:parallel}
%=======================================================================

Specify parallelism type and parameters.  For example, non-blocking
MPI, MPI-2, hybrid MPI/UPC, performance-related parameters such as
buffer size, etc.

Specifiy parallelism and parameters

\begin{itemize}
\item MPI (send/recv and type, one-sided and type, what level)
\item OpenMP (num threads, what level)
\item UPC (num threads, what level)
\item pthreads (num threads, what level)
\item cooperative parallelism
\item levels for each if multiple
\end{itemize}

%=======================================================================
\section{Error recovery parameters} \label{s:error}
%=======================================================================

Fault tolerance and adaptivity parameters

\begin{itemize}
\item fault tolerance methodology
\item adaptivity
\end{itemize}

%=======================================================================
\section{Performance monitoring parameters} \label{s:performance}
%=======================================================================

Performance monitoring and optimization(?) parameters

%=======================================================================
\chapter{Outputs} \label{s:inputs}
%=======================================================================

%=======================================================================
\chapter{Functional Requirements}
%=======================================================================

%=======================================================================
\section{Control}
%=======================================================================

%=======================================================================
\section{Physics}
%=======================================================================

%=======================================================================
\subsection{Hydrodynamics}

%=======================================================================
\subsection{Self-gravity}

%=======================================================================
\subsection{MHD}

%=======================================================================
\subsection{RT}

%=======================================================================
\section{Data-Structures}
%=======================================================================

%=======================================================================
\subsection{Fields}

%=======================================================================
\subsection{Particles}

%=======================================================================
\subsection{Structured Adaptive Mesh Hierarchies}

%=======================================================================
\subsection{Continuous Adaptive Mesh Hierarchies}

%=======================================================================
\section{Parallelism}
%=======================================================================

Hardware platform parallelism will be considered to be multilevel,
including nodes, processors, and cores.  Computational tasks will be
flexibly organized into hierarchical levels to aid mapping to multiple
hardware parallelization levels, and task sizes in different levels
will allow flexibility to help optimize granularity for the different
given parallelization level components.



Flexible parallelism paradigms: map parallelism to tasks

Automatic code generation (or other) of parallel tasks to implement
parallelism and optimize performance

%=======================================================================
\subsection{MPI Send/Recv}

%=======================================================================
\subsection{MPI2 Get}

%=======================================================================
\subsection{OpenMP}

%=======================================================================
\subsection{Collaberative parallelism}

%=======================================================================
\subsection{Pipelining}



\appendix

\chapter{\enzo\ parameter list}

\small
\begin{tabular}{lll}
\todo\ \code{AdjustUVBackground} &
\todo\ \code{BaryonSelfGravityApproximation} &
\todo\ \code{BoundaryConditionName} \\
\todo\ \code{CellFlaggingMethod} &
\todo\ \code{ComovingCoordinates} &
\todo\ \code{ComputePotential} \\
\todo\ \code{ConservativeInterpolation} &
\todo\ \code{CoolDataParamfile} &
\todo\ \code{CourantSafetyNumber} \\
\todo\ \code{CubeDumpEnabled} &
\todo\ \code{CubeDump} &
\todo\ \code{CycleLastDataDump} \\
\todo\ \code{CycleLastHistoryDump} &
\todo\ \code{CycleLastRestartDump} &
\todo\ \code{CycleSkipDataDump} \\
\todo\ \code{CycleSkipGlobalDataDump} &
\todo\ \code{CycleSkipHistoryDump} &
\todo\ \code{CycleSkipRestartDump} \\
\todo\ \code{DataDumpDir} &
\todo\ \code{DataDumpName} &
\todo\ \code{DataDumpNumber} \\
\todo\ \code{DataLabel} &
\todo\ \code{DataUnits} &
\todo\ \code{DomainLeftEdge} \\
\todo\ \code{DomainRightEdge} &
\todo\ \code{dtDataDump} &
\todo\ \code{dtHistoryDump} \\
\todo\ \code{dtMovieDump} &
\todo\ \code{dtRestartDump} &
\todo\ \code{dtTracerParticleDump} \\
\todo\ \code{DualEnergyFormalismEta1} &
\todo\ \code{DualEnergyFormalismEta2} &
\todo\ \code{DualEnergyFormalism} \\
\todo\ \code{ExternalBoundaryIO} &
\todo\ \code{ExternalBoundaryTypeIO} &
\todo\ \code{ExternalBoundaryValueIO} \\
\todo\ \code{ExtractFieldsOnly} &
\todo\ \code{FluxCorrection} &
\todo\ \code{GadgetEquilibriumCooling} \\
\todo\ \code{Gamma} &
\todo\ \code{GlobalDir} &
\todo\ \code{GravitationalConstant} \\
\todo\ \code{GravityBoundaryFaces} &
\todo\ \code{GravityBoundaryName} &
\todo\ \code{GravityBoundaryRestart} \\
\todo\ \code{GravityResolution} &
\todo\ \code{GreensFunctionMaxNumber} &
\todo\ \code{GreensFunctionMaxSize} \\
\todo\ \code{GridVelocity} &
\todo\ \code{HistoryDumpDir} &
\todo\ \code{HistoryDumpName} \\
\todo\ \code{HistoryDumpNumber} &
\todo\ \code{huge\_number} &
\todo\ \code{HydroMethod} \\
\todo\ \code{InitialCPUTime} &
\todo\ \code{InitialCycleNumber} &
\todo\ \code{Initialdt} \\
\todo\ \code{InitialTime} &
\todo\ \code{InterpolationMethod} &
\todo\ \code{LeftFaceBoundaryCondition} \\
\todo\ \code{LocalDir} &
\todo\ \code{MaximumGravityRefinementLevel} &
\todo\ \code{MaximumParticleRefinementLevel} \\
\todo\ \code{MaximumRefinementLevel} &
\todo\ \code{MaximumSubgridSize} &
\todo\ \code{MinimumEfficiency} \\
\todo\ \code{MinimumEnergyRatioForRefinement} &
\todo\ \code{MinimumMassForRefinement} &
\todo\ \code{MinimumMassForRefinementLevelExponent} \\
\todo\ \code{MinimumOverDensityForRefinement} &
\todo\ \code{MinimumPressureJumpForRefinement} &
\todo\ \code{MinimumPressureSupportParameter} \\
\todo\ \code{MinimumShearForRefinement} &
\todo\ \code{MinimumSlopeForRefinement} &
\todo\ \code{MinimumSubgridEdge} \\
\todo\ \code{MovieDataField} &
\todo\ \code{MovieDumpDir} &
\todo\ \code{MovieDumpName} \\
\todo\ \code{MovieDumpNumber} &
\todo\ \code{MovieRegionLeftEdge} &
\todo\ \code{MovieRegionRightEdge} \\
\todo\ \code{MovieSkipTimestep} &
\todo\ \code{MultiMetals} &
\todo\ \code{MultiSpecies} \\
\todo\ \code{MustRefineParticlesRefineToLevel} &
\todo\ \code{NewMovieDumpNumber} &
\todo\ \code{NewMovieLeftEdge} \\
\todo\ \code{NewMovieName} &
\todo\ \code{NewMovieParticleOn} &
\todo\ \code{NewMovieRightEdge} \\
\todo\ \code{NumberOfBufferZones} &
\todo\ \code{NumberOfParticleAttributes} &
\todo\ \code{NumberOfParticles} \\
\todo\ \code{OutputFirstTimeAtLevel} &
\todo\ \code{ParallelParticleIO} &
\todo\ \code{ParallelRootGridIO} \\
\todo\ \code{ParticleBoundaryType} &
\todo\ \code{ParticleCourantSafetyNumber} &
\todo\ \code{ParticleTypeInFile} \\
\todo\ \code{ParticleTypeInFile} &
\todo\ \code{PartitionNestedGrids} &
\todo\ \code{PointSourceGravityConstant} \\
\todo\ \code{PointSourceGravityCoreRadius} &
\todo\ \code{PointSourceGravity} &
\todo\ \code{PointSourceGravityPosition} \\
\todo\ \code{PPMDiffusionParameter} &
\todo\ \code{PPMFlatteningParameter} &
\todo\ \code{PPMSteepeningParameter} \\
\todo\ \code{PressureFree} &
\todo\ \code{ProblemType} &
\todo\ \code{RadHydroParamfile} \\
\todo\ \code{RadiationFieldLevelRecompute} &
\todo\ \code{RadiationFieldType} &
\todo\ \code{RadiationHydrodynamics} \\
\todo\ \code{RadiationSpectrumNormalization} &
\todo\ \code{RadiationSpectrumSlope} &
\todo\ \code{RadiativeCooling} \\
\todo\ \code{RandomForcingEdot} &
\todo\ \code{RandomForcing} &
\todo\ \code{RandomForcingMachNumber} \\
\todo\ \code{RedshiftDumpDir} &
\todo\ \code{RedshiftDumpName} &
\todo\ \code{RefineBy} \\
\todo\ \code{RefineByJeansLengthSafetyFactor} &
\todo\ \code{RefineRegionLeftEdge} &
\todo\ \code{RefineRegionRightEdge} \\
\todo\ \code{RestartDumpDir} &
\todo\ \code{RestartDumpName} &
\todo\ \code{RestartDumpNumber} \\
\todo\ \code{RightFaceBoundaryCondition} &
\todo\ \code{S2ParticleSize} &
\todo\ \code{SelfGravity} \\
\todo\ \code{SetHeIIHeatingScale} &
\todo\ \code{SetUVBAmplitude} &
\todo\ \code{SRBprefix} \\
\todo\ \code{StarEnergyToQuasarUV} &
\todo\ \code{StarEnergyToStellarUV} &
\todo\ \code{StarEnergyToThermalFeedback} \\
\todo\ \code{StarMakerMassEfficiency} &
\todo\ \code{StarMakerMinimumDynamicalTime} &
\todo\ \code{StarMakerMinimumMass} \\
\todo\ \code{StarMakerOverDensityThreshold} &
\todo\ \code{StarMassEjectionFraction} &
\todo\ \code{StarMetalYield} \\
\todo\ \code{StarParticleCreation} &
\todo\ \code{StarParticleFeedback} &
\todo\ \code{StaticHierarchy}
\end{tabular}

\begin{tabular}{lll}
\todo\ \code{StaticRefineRegionLeftEdge} &
\todo\ \code{StaticRefineRegionLevel} &
\todo\ \code{StaticRefineRegionRightEdge} \\
\todo\ \code{StopCPUTime} &
\todo\ \code{StopCycle} &
\todo\ \code{StopFirstTimeAtLevel} \\
\todo\ \code{StopTime} &
\todo\ \code{TimeActionParameter} &
\todo\ \code{TimeActionParameter} \\
\todo\ \code{TimeActionRedshift} &
\todo\ \code{TimeActionRedshift} &
\todo\ \code{TimeActionTime} \\
\todo\ \code{TimeActionTime} &
\todo\ \code{TimeActionType} &
\todo\ \code{TimeLastDataDump} \\
\todo\ \code{TimeLastHistoryDump} &
\todo\ \code{TimeLastMovieDump} &
\todo\ \code{TimeLastRestartDump} \\
\todo\ \code{TimeLastTracerParticleDump} &
\todo\ \code{tiny\_number} &
\todo\ \code{TopGridDimensions} \\
\todo\ \code{TopGridGravityBoundary} &
\todo\ \code{TopGridRank} &
\todo\ \code{TracerParticleDumpDir} \\
\todo\ \code{TracerParticleDumpName} &
\todo\ \code{TracerParticleDumpNumber} &
\todo\ \code{TracerParticleOn} \\
\todo\ \code{UniformGravityConstant} &
\todo\ \code{UniformGravityDirection} &
\todo\ \code{UniformGravity} \\
\todo\ \code{Unigrid} &
\todo\ \code{UseMinimumPressureSupport} &
\todo\ \code{VersionNumber} \\
\todo\ \code{WritePotential} &
\todo\ \code{ZEUSLinearArtificialViscosity} &
\todo\ \code{ZEUSQuadraticArtificialViscosity}
\end{tabular}

\end{document}

%==================================================================
