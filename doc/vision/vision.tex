%0       1         2         3         4         5         6         7         8
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%=======================================================================
\documentclass{article}
%=======================================================================

\include{include}

%=======================================================================

\begin{document}

%=======================================================================
\TITLE{Vision and Scope Document}{James Bordner}{$Rev$}
%=======================================================================

\section{Problem Statement}
\subsection{Project background}


    The \cello\ Project is designed to be \enzo: The Next Generation.
    \enzo\ was initially conceived in the early 1990's by Michael
    L.~Norman and Greg Bryan to be a multi-resolution astrophysics and
    cosmology application, implemented using structured (patch-based)
    adaptive mesh refinement (SAMR).  It incorporated a modified
    high-order Piecewise Parabolic Method (PPM) solver for
    hydrodynamics, and a Particle-Mesh (PM) method for dark matter
    dynamics.

    Greg Bryan began coding \enzo\ in the Fall of 1994.  He
    implemented it in C++ and Fortran 77, using primarily a procedural
    (structured) programming paradigm.  Initially he targeted shared
    memory machines such as the SGI Origin, and later added support
    for distributed memory machines via the Message Passing Interface
    (MPI).  Beginning around 2000, other developers began to modify
    \enzo\ in earnest, especially Robert Harkness, who added support
    for 64-bit integers, packed AMR output, and greatly improved
    the scalability of \enzo\ to TeraScale machines.

    \enzo\ was developed without any supporting documents, such as
    requirement specifications, design description, or test document.
    It was also written without any explicit coding standard, though
    the original author closely followed implicit coding guidelines,
    such as including uniform file header comment blocks and using
    descriptive variable and function names.  The original author also
    implemented many application tests to help ensure the accuracy
    of the computed solutions.

\subsection{Stakeholders}

To clarify roles among stakeholders, we assign ``effective'' titles.

\begin{itemize}
%
 \item  Prof.~Michael L.~Norman, effective \textit{Chief Executive Officer}
%
 \item Prof.~Greg Bryan, effective \textit{Chief Science Officer}.
 Since \cello\ is intended to be the successor to \enzo, it is
 important that \cello\ maintains the same high scientific standards
 set by the original application developed by Greg.
%
 \item Dr.~Robert Harkness, effective \textit{Chief Technology
 Officer}.  Robert is the most knowledgeable of the latest enhancements to
 \enzo, and he has the deepest understanding of how best to take advantage of
 PetaScale platforms
%
\item The Development Team:
\begin{itemize}
\item Dr.~James Bordner (Team Leader, OOP design, documentation)
\item Dr.~Robert Harkness (low-level design, scalability, performance)
\item Rick Wagner (unit testing and analysis specialist)
\item Dr.~Alexei Kritsuk (hydrodynamics and turbulence specialist)
\item David Collins (MHD specialist)
\item Dr.~Dan Reynolds (RHD specialist)
\item Prof.~Brian O'Shea (star formation specialist)
\item Dr.~Paschal Paschos (chemistry and cooling specialist)
\item Stephen Skory (quality assurance and application testing)
\end{itemize}
\end{itemize}
    

\subsection{Users}


    Users can be grouped into several categories: core developers,
    collaborrators, and community users.

    \textbf{Core users} are the scientists and graduate students in
    the Laboratory for Computational Astrophysics.  Most core users
    will also participate in development of \cello, and virtually all
    developers (except the \cello\ team leader Bordner) will be core
    users.  Core users include Norman, Kritsuk, Padoan, Harkness,
    Collins, Paschos, Wagner, and Skory.

    \textbf{Collaborative users} include all scientists not currently
    in the Laboratory for Computational Astrophysics, but who will use
    \cello\ and coauthors papers with current LCA members.
    Collaborative users may include Tom Abel, John Hayes, Bryan
    O'Shea, David Tytler, Dan Reynolds.

    \textbf{Community users} are all users not in the previous two
    groups.  We expect that \cello\ functionality will be driven
    primarily by the needs of the core and collaborative users, since
    they are fairly representative of general community users.
    However, \cello\ will be designed specifically to be a community
    code, so must meet the needs of community users as a whole.
    
\subsection{Risks}

Uncertain future of development team members (especially graduate students nearing graduation)

Uneven capabilities of development team members (especially software development methodology)

Uncertain future parallel platform characteristics

Future platforms may have difficult to program accelerator cards

Future platforms may have limited memory per shared memory node

May become platform dependent: IBM Blue Waters

Political pressure to use CHARM++

Political pressure to use UPC

Discontinued or uncertain future development of dependent libraries

\code{lcaperf}
\code{PAPI}
\code{hypre}
\code{P3DFFT}
\code{SPRNG}
\code{VisIt}
\code{arprec}

Uncertain future usability of parallel languages

\code{MPI}
\code{OpenMP}
\code{UPC}
   


\subsection{Assumptions}

    \textit{This is the list of assumptions that the stakeholders, users, or
    project team have made. Often, these assumptions are generated
    during a Wideband Delphi estimation session (see Chapter 3). If
    Wideband Delphi is being used, the rest of the vision and scope
    document should be ready before the Delphi meeting and used as the
    basis for estimation. The assumptions generated during the
    estimation kickoff meeting should then be reviewed, and any
    nontechnical assumptions should be copied into this
    section. (Technical assumptions---meaning assumptions that affect
    the design and development but not the scope of the
    project---should not be included in this document. The estimate
    results will still contain a record of these assumptions, but they
    are not useful for this particular audience.)}

\section{Vision of the Solution}

\subsection{Vision statement}

    \textit{The goal of the vision statement is to describe what the project
    is expected to accomplish. It should explain what the purpose of
    the project is. This should be a compelling reason, a solid
    justification for spending time, money, and resources on the
    project. The best time to write the vision statement is after
    talking to the stakeholders and users and writing down their
    needs; by this time, a concrete understanding of the project
    should be starting to jell.}

The purpose of the \cello\ project is to provide an open source
software application for high-performance computational astrophysics
and cosmology.  It will be used both as a testbed for experimenting
with new software organization, parallelization, distributed
datastructure, algorithm, and implementation techniques, as well as
for enabling cutting-edge astrophysics and cosmological science
simulations on the largest parallel high performance computers
available.  Objectives are to reliably provide high-quality numerical
solutions, computed by distributing the workload efficiently across
$100,000$ to $1,000,000$ computational floating-point units, while
maintaining a high level of utilization of available computational
resources.

\devel{Physics.}
The scope of physics capabilities are representations of baryonic and
dark matter, hydrodynamics, self-gravity, radiative cooling,
cosmological expansion, star formation, radiative cooling,
multi-species chemistry, magnetohydrodynamics, and radiation transfer.
diation transfer.

\devel{AMR.}  Computations will be performed at multiple spacial
   and temporal resolutions using adaptive mesh refinement (AMR).
   This will permit the physics modules to capture the full range of
   scales of interest, but without excessive computation and memory
   storage.  Methods and data-structures will be designed to maximize
   scalability in both computation and memory usage.  Characteristics
   of the AMR hierarchies, e.g.~grid patch characteristics (size, size
   quantization, shape, etc.) will be flexible to allow for
   optimization of refinement efficiency, parallel task sizes, and
   task computation efficiency.

   \devel{Parallelization levels.}  To take advantage of the
   hierarchical parallel nature of modern supercomputers, \cello\ will
   support multiple levels of parallelization in its methods and
   datastructures.  This will include the coarse-grain parallelism to
   concurrently run simulations in an ensemble at the node or
   supernode level, medium-grain parallelism to evolve grid patches or
   particle groups of a simulation concurrently across a distributed
   memory subsystem at the node or socket level, and fine-grain parallelism to
   evolve grid cells within grid patches or particles within a group
   using shared-memory parallelism at the socket or core level.

   \devel{Parallelization types.}  This parallelization will be
   modularized (as much as technologically feasible) to improve
   flexibility in choosing the best method(s) of parallelization for a
   given problem on a given parallel platform.  MPI (two-sided), MPI2
   (one-sided), OpenMP, and possibly UPC support will be included, as
   well as flexibility in choosing hybrid schemes such as MPI +
   OpenMP, or MPI + UPC.  Other schemes based on shared memory array
   libraries or POSIX pthreads may also be considered.
   Parallelization methods will be primarily data parallelism,
   supporting both distributed memory and shared memory in isolation
   or combination.  Support for collaberative (functional) parallelism
   and pipelining will also be considered.

%  load balancing

   \devel{Dynamic load balancing.}  Multiple levels of parallel tasks
   will be load balanced using hierarchical dynamic load balancing
   algorithms.  Load balancing schemes will use dynamically measured
   performance data gathered by the running simulation to make load
   balancing decisions, and will allow flexibility in optimizing the
   parallel distribution of computation, memory storage, or a
   combination of both.  Combining flexible hierarchical
   parallelization schemes with hierarchical load balancing
   algorithms, together with scalable methods and efficient AMR
   data-structures, are expected to lead to high parallel efficiency
   and scalability.

% Field data layout: vertical data movement optimization

   \devel{Deep memory hierarchies.}  
%
   The properties of individual grid patches, and details of how
   scalar and vector fields are stored within grid patches, will be
   flexible to permit optimizing the use of deep memory/cache
   hierarchies.  This includes hierarchical blocking of grid data to
   maximize reuse of data in caches, padding of arrays to reduce cache
   thrashing effects for low-associativity caches, and interleaving
   vector field data or select scalar fields to improve data locality.
   These capabilities, together with efficient methods and
   implementation of computations, are expected to lead to high
   single-thread computational efficiency and data movement through
   memory/cache hierarchies.

%  Performance monitoring

   \devel{Performance monitoring.}  
%
   Global performance-related measurements will be continuously
   collected for simulations.  This will include parallel
   communication amount, rates, and time; memory and computational
   load balance efficiency; memory usage and reference rates; floating
   point operation counts and rates; disk storage rates and amounts,
   and time.

    \textbf{Efficient on PetaScale platforms.}

    \textbf{Easily Enhanceable.}

    \textbf{Instructive.}  

    \textbf{Powerful.}

    \textbf{Not overly dependent on external libraries.}

    % Enzo not OOP % Enzo overly complex % A fundamental flaw of
    \enzo\ is that it is continually modified, yet not designed to be
    modified.  It was written primarily using the structured
    programming paradigm, which is known to be effective for designing
    and implementing large-scale applications, but is not ideal for
    subsequent redesign and modification.  As \enzo\ has been
    repeatedly modified by successive researchers and students, its
    complexity has increased rapidly, in particular much faster than
    the inherent complexity of the underlying functionality being
    implemented.
%
    \enzo\ code is tightly coupled, has much repetition, is difficult
    to read and understand.

    % Enzo must be modified continually
    % 1. Evolving supercomputers
    % 2. Progress in science
    Yet \enzo\ is an application that must be modified continually,
    for at least two reasons.  First, because rapidly evolving
    supercomputer platforms put ever-increasing demands on application
    scalability and efficiency; and second, because a primary usage
    model is physics researchers and graduate students who want to
    solve new physics problems, which necessarily requires adding new
    computational physics capabilities.  Such capabilities include
    radiative transfer, magnetohydrodynamics, and turbulence modeling.

    % Enzo input files
    % flat list
    % limited expressibility
    % inherently limits power of application
% 
    \enzo\ currently uses a large ``flat'' list of input parameters.
    Because they are not organized explicitly into sections, it can be
    difficult for users to learn and write parameter files.
%
    Also, parameters have limited expressibility---each parameter is
    typically an integer with a small number of valid values.
%
    The limited input file grammar inherently limits the power of
    \enzo\ itself.  For example, input control is not sufficiently
    powerful to define a new test problem that is not already
    implemented explicitly in code.  Running a new test problem
    requires enhancing the \enzo\ codebase.


\subsection{List of features}

    \textit{This section contains a list of features. A feature is as a
    cohesive area of the software that fulfills a specific need by
    providing a set of services or capabilities. Any software
    package---in fact, any engineered product---can be broken down
    into features. The project manager can choose the number of
    features in the vision and scope document by changing the level of
    detail or granularity of each feature, and by combining multiple
    features into a single one. Sometimes those features are small
    ("screw-top cap," "holds one liter of liquid"); sometimes they are
    big ("four-wheel drive," "seats seven passengers"). It is useful
    to describe a product in about 10 features in the vision and scope
    document , because this usually yields a level of complexity that
    most people reading it are comfortable with. Adding too many
    features will overwhelm most readers.}

    \textit{Each feature should be listed in a separate paragraph or bullet
    point. It should be given a name, followed by a description of the
    functionality that it provides. This description does not need to
    be detailed; it can simply be a few sentences that give a general
    explanation of the feature. However, if there is more information
    that a stakeholder or project team member feels should be
    included, it is important to include that information. For
    example, it is sometimes useful to include a use case (see Chapter
    6), as long as it is written in such a way that all of the
    stakeholders can read and understand it.}

\subsection{Scope of phased release (optional)}

    \textit{Sometimes software projects are released in phases: a version of
    the software with some subset of the features is released first,
    and a newer, more complete version is released later. This section
    describes the plan for a phased release, if that approach is to be
    taken.}

    \textit{This is useful when there is an important deadline for the
    software, but developing the entire software project by that
    deadline would be unrealistic. The most common way to compromise
    on this release date is to divide the features into two or more
    releases. In that case, this section should identify specifically
    when those versions will be released, and which features will be
    included in each version. It's reasonable to divide one feature up
    between two releases, as long as it is made clear exactly how that
    will happen.}

    \textit{If a project manager needs to release a project in phases, it is
    critical that the project team be consulted. Some features are
    much more difficult to divide than others, and the engineers might
    see dependencies between features that are not clear to the
    stakeholders and project manager. After the phased release plan is
    written down and agreed upon, the project team should always be
    asked to re-estimate the effort and a new project plan should be
    generated (see below). This will ensure that the phased release is
    feasible and compatible with the organization's priorities.}

\subsection{Features that will not be developed}

    \textit{Features are often left out of a project on purpose. When a
    feature is explicitly left out of the software, it should be added
    to this section to tell the reader that a decision was made to
    exclude it. For example, one way to handle an unrealistic deadline
    is by removing one or more features from the software, in which
    case the removed features should be moved into this section. The
    reason these features should be moved rather than deleted from the
    document is that otherwise, readers might assume that they were
    overlooked and bring them up in a review. This is especially
    important during the review of the document because it allows
    everyone to agree on the exclusion of the feature (or object to
    it).}

\end{document}

%==================================================================
