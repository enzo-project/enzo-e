%0       1         2         3         4         5         6         7         8
%2345678901234567890123456789012345678901234567890123456789012345678901234567890

%=======================================================================
\documentclass[14pt,letter]{article}
%=======================================================================
\include{include}

% Project summary
% Introduction
%    Extreme parallelism
%    Extreme AMR
%    Existing AMR frameworks
%       PARAMESH
%       Chombo
%       SAMRAI
% Software requirements
% Software design
%    High level components
%    Data structures
%       Patch coalescing  for ``shallow'' AMR
%       Targeted refinement with backfill for ``deep'' AMR
%    Task scheduling
%    Load balancing
% Implementation
%    Parallelism
%    Fault tolerance
%    Software implementation
% Development plan
% Milestones and deliverables

\usepackage{natbib}

%=======================================================================

\begin{document}

\tableofcontents
%\Large
% \renewcommand{\pargraph}[1]{}

\nocite{StSh09} % Scalability challenges for massively parallel {AMR} applications
\nocite{WiHy03} % Enhancing scalability of parallel structured {AMR} calculations
\nocite{GuWi06} % Parallel clustering algorithms for structured {AMR}
\nocite{BuGh08} % Towards adaptive mesh {PDE} simulations on petascale computers
\nocite{BaBu09} % MPI on a million processors

\nocite{LaTa06} % hierarchical load balancing

%=======================================================================
\TITLE{A Software Framework for Extreme Adaptive Mesh Refinement}{James Bordner}{$Rev$}
%=======================================================================

%
%
%

% Keywords
%
%    AMR
%    applied mathematics
%    collaborative software environments
%    computer science technologies
%    exascale
%    extreme scale
%    failure avoidance
%    failure effects avoidance
%    fault tolerance
%    GPU
%    hierarchical
%    HPC
%    memory hierarchy
%    multicore
%    multiphysics
%    multiscale
%    one-sided communication
%    performance tuning
%    petascale
%    PGAS
%    research goals
%    resilient software
%    UPC
%    virtual processes
%
% suggestions
%
%    preliminary results
%    good references critical; only give great references
%    use rfp keywoards
%    sustainability--what happens when money ends
%    sales document not technical manual
%    visit agency web site for similar funding
%    make benefits clear
%    write for evaluators
%    justify and support all claims
%    active voice: strong subjects and active verbs
%    first paragraph sentence conveys topic
%    65 characters per line
%    duplicate information instead of cross reference
%    use graphics and tables
%    short text with bullets
%    stay on topic
%
% more suggestions
%
% [ compelling, organization goals and success, relevence to agency]
% problem statement--needs assessment
%     motivation / history
%       enzo
%       cello project
%       decoupled framework
%       well-aware of scalability issues
%       understand importance of performance / scalability of all components
%     purpose
%     beneficiaries
%     what is being done
%     what we will do
% objectives: table with direct items
% development plan
%    chart with timelines
%    decision points
%    milestones
%    tasks
% methods and design
%     use flow charts and diagrams to break up text
%     justify course of action
%     highlight innovative features
% past experience
% management approach
%   quality control
%   emphasize performance management
%   technological systems in place


%=======================================================================
\section{Project Summary}  \label{s:summary}
%=======================================================================


\ \\ \pargraph{Proposal} 
%
We propose to develop a new parallel software framework for adaptive
mesh refinement (AMR).  
%
A distinguishing characteristic relative to existing AMR libraries and
frameworks is the aggressive pursuit of extreme scalability, with
respect to both hardware parallelism and software data structures.
%
The design will be forward-looking, targeted not just at the largest
existing supercomputers, but also future HPC platforms as they evolve
through the petascale era and into the exascale.
%
The proposed AMR framework will enable application developers to
write multiphysics applications for simulating phenomena on an
unprecidented range of spacial and temporal scales.


Parallelization will be primarily data parallel, with task
parallelization also available to augment the data parallelism if
desired.  A wide variety of parallelization technologies will be
supported from the start, including both one- and two-sided message
passing via the MPI~\cite{@@@MPI} library, the partitioned global
address space (PGAS) approach via the UPC~\cite{@@@UPC} language,
shared memory parallel programming using OpenMP compiler directives,
and the processor virtualization approach provided by the CHARM++
framework~\cite{@@@CHARM}.  Select hybrid approaches will also be
supported, including MPI with OpenMP, and MPI with UPC.  This will be
accomplished by encapsulating the parallelization technologies within
low-level distributed memory and shared memory API's.


\ \\ \pargraph{data structure scope summary}
%
The AMR data structures will be implemented using a fully distributed
generalization of the octree data structure.  Tree nodes will be
associated with both logically Cartesian grids and particles, so both
Eularian and Lagrangian (and hybrid) methods can be implemented.

\ \\ \pargraph{extreme scalability} The general approach for pursuing
extreme scalability for both parallelism and data structures is by
aggressively localizing the individual AMR patch / block tasks by removing
all unnecessary global communication.  This
will help control data structure scalability issues related to limited
floating-point precision and range, as well as reducing the need for
global synchronization and reduction operations.

\ \\ \pargraph{fault tolerance} Fault tolerance and software
resilience are also crucial factors at extreme scales.  The
checkpoint / restart to disk paradigm is known to be ultimately unscalable, so we will Our framework
will be designed to be resistent to memory, compute component, network,
disk, and software failures., since it has been
observed that frequency of failures is proportional to the number of
sockets. CITE Fault tolerance Approaches to fault tolerancedue to the
reduced MTTI (mean time to interrupt) , since the probability that
hardware component, operating system, library, or application
component fails Unfortunately, approaches to fault tolerance are still

@@@@@@@@@@@@@@@@@@@@

\ \\ \pargraph{Intellectual Merit.}


\ \\ \pargraph{parallelization strategy}
%

\pargraph{load balancing}
%
Dynamic load balancing of parallel tasks will be hierarchical, which
will improve mapping of the software data structures to the
hierarchical components of the target computational platforms (i.e.
compute nodes, sockets, cores).~\cite{@@@LAN-DLB}.
%
% integer range
%
%
\ \\ \pargraph{I/O}
Disk I/O is also a crucial factor for petascale 
%
\ \\ \pargraph{availablity} We plan to make this framework publicly
available for scientific research use.
%
\ \\ \pargraph{deliverables} 
The result of this project will be software framework capable of
allowing application developers write extremely scalable
multi-resolution multi-physics applications.

%=======================================================================
\section{Introduction} \label{s:intro}
%=======================================================================


The \cello\ Project began as an effort to redesign the parallel data
structures of the astrophysics and cosmology application \enzo\
\cite{@@@ENZO}.  \enzo\ was conceived in the early 1990's by Michael
L.~Norman and Greg Bryan, and implemented using structured
(patch-based) adaptive mesh refinement (SAMR) \cite{@@@SAMR}.  It
incorporated a modified high-order Piecewise Parabolic Method (PPM)
solver \cite{@@@PPM} for hydrodynamics, and a Particle-Mesh (PM)
method \cite{@@@PM} for dark matter dynamics.  So far in its 15 year
lifetime, \enzo\ simulations have led to important scientific
contributions to the fields of astrophysics, cosmology, and
turbulence.

\enzo\ scales well up to $\approx 10^4$ to $\approx 10^5$ processes,
depending on the problem.  Efforts to improve its scalability beyond
this have become increasingly difficult, due to the increasing
invasiveness of the desired changes and the increasing complexity of
the code base.  The \cello\ Project aims to improve on \enzo\ in three
ways: improved software development process, object-oriented design
and implementation, and overhauled distributed AMR infrastructure.  The
physics capabilites, which are continually being improved and refined in \enzo, will be retained in \enzo II



@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \ \\

\begin{verbatim}
Motivation
Forward-looking
Supercomputer trends
Innovative / core ideas
  hierarchical load balancing
  separate scheduling of communication and computation
  flexible dynamic data organization for comm, comp, disk
  task size / count control
  gang scheduling for GPU support
  task parallelism: pipeline computational methods to increase parallelism
  AMR: patch coalescing
  AMR: targeted refinement by 4, 8 with backfill
  aggresive localization
    only store local coordinates for particles
    only store local patch connectivity
  fault tolerance @@@
  multiple parallelization technologies with flexible hybrid support
     why?  
        experimentally 
        forces encapsulation of parallelism; 
        likely improves ease of adapting future parallel strategies
  designing with extreme scale in mind from the beginning
\end{verbatim}

\begin{verbatim}
upcoming systems
   BW
   ...
   ...
   core counts
   extrapolate top500 10 years
   what this means for extreme scalable AMR + particles
\end{verbatim}

\begin{verbatim}
refined  / borrowed ideas
  fully distributed AMR datastructure
  patch block registers for improved computing / user interface flexibility
  communication flux registers (\code{Chombo}) for communication
  guard cells allocated when needed
  localized fully-adaptive time-stepping (no global reductions) (refined)
\end{verbatim}


\note{Extreme parallelism}

\begin{verbatim}
distributed data placement
   hierarchical
   different needs for different tasks
      compute
      store
\end{verbatim}

\begin{verbatim}
parallel computation
   data parallelism (SPMD)
   task parallelism
   cooperative parallism (MPMD)
\end{verbatim}

\begin{verbatim}
parallel technology
   MPI / CHARM++ / UPC / OpenMP / GPU
\end{verbatim}

\begin{verbatim}
heterogeneous hardware computation
   GPUs, cores
\end{verbatim}

\begin{verbatim}
load balancing
   keep all cores busy
   between shared memory compute nodes -- memory
   within shared memory compute nodes -- computation
\end{verbatim}

\begin{verbatim}
task definition
   size
   count
   hierarchical
\end{verbatim}

\begin{verbatim}
task scheduling
   hierarchical / multiscale
     distributed versus shared memory
   communication / computation
   decouple data distribution from task scheduling
\end{verbatim}

\begin{verbatim}
reducing synchronization / increasing task independence
\end{verbatim}

\begin{verbatim}
software resiliency / fault tolerance
\end{verbatim}

\begin{verbatim}
checkpointing / restart
   trend: disk progressively slower
\end{verbatim}


\pargraph{load balancing summary}
We will use a hierarchical dynamic load balancing approach, with
support for balancing with respect to multiple metrics (memory use,
computational load, etc.) for different hierarchical computational
components (compute node, socket, etc.), with different rebalancing
frequencies for different levels.

\note{Extreme AMR}

\begin{verbatim}
parallel scalability issues
   global synchronization / reductions
   load balancing
   I/O
   fault tolerance
\end{verbatim}

\begin{verbatim}
AMR-specific parallel scalability issues
   elliptic
   remeshing--especially SAMR
   load balancing
   timestep determination
\end{verbatim}

\begin{verbatim}
AMR datastructure scalability
   breadth--number of grid patches
   depth--range of multi-resolution
   floating point precision issues
   floating point range issues
   integer range issues
\end{verbatim}

%-----------------------------------------------------------------------
\subsection{Existing AMR frameworks} \label{ss:review}
%-----------------------------------------------------------------------

\begin{verbatim}
 http://people.sc.fsu.edu/~tomek/AMR/index.html
 *   Adaptive Mesh Refinement Software for Hyperbolic Conservation Laws, Marsha Berger
 * Adaptive Mesh Refinement and Parallel Computing, John A. Trangenstein, Mathematics Department, Duke University.
 * Amrita, James Quirk.
 * AMR1D, a simple adaptive mesh refinement code for solving the one-dimensional Euler equations on an unstructured grid.
 * AMRCART, The AMRCART MHD Code, Rolf Walder.
 * AMRCLAW, Adaptive Mesh Refinement + CLAWPACK. AMRCLAW is a joint project between Randy LeVeque and Marsha Berger.
 * AMRMHD3D, 3-D AMR code with FCT MHD solver.
 * AMROC, Blockstructured Adaptive Mesh Refinement in object-oriented C++, Ralf Deiterding.
 * AMRPoisson, Solving Poisson's Equation using Adaptive Mesh Refinement (AMR), Computational Fluid Dynamics, Dept. of Mechanical Engineering, U.C. Berkeley.
 * ARMS, 3-D AMR code with FCT MHD solver.
 * BEARCLAW, general purpose software package for solving time-dependent partial differential equations, Sorin Mitran, Applied Mathematics Program, The University of North Carolina at Chapel Hill.
 * BSAMR, Block Structured Adaptive Mesh Refinement, Daniel D. Wake, College of Engineering, UC Davis.
 * Cactus, The Cactus Code Server, Max Planck Institute for Gravitational Physics (Albert Einstein Institute), Max Planck Society.
 * Cart3D, Michael J. Aftosmis, Numerical Aerospace Simulation (NAS) Division Systems, NASA Ames Research Center
 * CTH, 3-D multi-material Eulerian AMR code from Sandia (export controlled).
 * Enzo, Cosmological Simulation Code, Laboratory for Computational Astrophysics, UCSD.
 * FEMLAB, FEM with automatic error control for two-dimensional convection-diffusion-absorption problems.
 * FLASH, multi-physics 3-D octree AMR parallel code.
 * Gerris Flow Solver, incompressible fluid flow with arbitrarily complex solid boundaries and adaptive mesh refinement, Stephane Popinet.
 * GrACE, Grid Adaptive Computational Engine, Manish Parashar.
 * NIRVANA, 3-D MHD AMR code, Udo Ziegler.
 * PARAMESH, 3-D octree AMR parallel code.
 * RACOON, Institute for Theoretical Physics I, University of Bochum.
 * SARA, A Solution Adaptive Remeshing Algorithm for structured grids, Dave Banks, Center for CFD, Dept. of Mech & Aero Eng, UC Davis.
 * Solution-Adaptive Grids, W. M. Keck Foundation CFD laboratory, University of Michigan.
 * SUUMA3D, Scalable Unstructured Mesh Algorithms and Applications, MCS ANL. 
\end{verbatim}

There are numerous AMR frameworks, libraries, and applications, each
with different design goals and decisions, data structures, AMR
algorithms, parallelization strategies, and parallel performance and
scaling.  Perhaps the top three most successful and closely related to
our proposed effort are PARAMESH (\cite{MaOl00} \nocite{OlMa05}
\nocite{Ol06}), \code{Chombo} (\cite{wwwchombo} \cite{CoGr09}), and SAMRAI
(@@@).  Other notable software include Clawpack (@@@), GrACe / DAGH
(@@@), ALPS (\cite{BuBu09}), and Carpet (@@@).

limitations of existing AMR libraries / frameworks

GADGET-2: Preeminent cosmological TreeSPH simulation code
DAGH (1998)

%-----------------------------------------------------------------------
\subsubsection{\code{PARAMESH}} \label{sss:paramesh}
%-----------------------------------------------------------------------

\nocite{wwwparamesh}
\nocite{MaOl00} % PARAMESH requested references
\nocite{OlMa05}
\nocite{Ol06}

The PARAMESH software was developed at the NASA Goddard Space Flight
Center and Drexel University.

Octree-based AMR


Due to a loss of funding PARAMESH can no longer be supported.

%-----------------------------------------------------------------------
\subsubsection{\code{Chombo}} \label{sss:chombo}
%-----------------------------------------------------------------------

\nocite{wwwchombo}
\nocite{CoGr09}

\code{Chombo} is in active development by the Applied Numerical Algorithms
Group of Lawrence Berkeley National Lab.  Its high-level design
consists of five loosely-coupled components based on a mathematical
decomposition: \code{BoxTools} for set calculus on point sets and
unions of rectangles, \code{AMRTools} for interprocess communication
between AMR levels, \code{AMRTimeDependent} for advancing the solution
using adaptive time-stepping, \code{AMRElliptic} for the
multigrid-based solution of elliptic AMR problems, \code{EBTools} for
embedded boundaries, and \code{ParticleTools} for particle processing
(not available in the current 3.0 version, according to the website
due to re-engineering of the component).

\code{CHOMBO} is very scalabale, and designed to run both hyperbolic and
elliptic problems on 10,000 processors.

Berger-Rigoutsos 

\pargraph{\code{Chombo} data distribution and load balancing}
Distributed data in \code{LayoutData}, \code{BoxLayoutData}, and
\code{LevelData} containers.  \code{Chombo} provides a load balancing
algorithm, and also allows the user to provide their own.


%-----------------------------------------------------------------------
\subsubsection{\code{SAMRAI}} \label{sss:samrai}
%-----------------------------------------------------------------------

[2001]

good scaling for numerical and data communication
poorer scaling in adaptive meshing and communication schedule construction 


%=======================================================================
\section{Software Requirements} \label{s:requirements}
%=======================================================================

\begin{verbatim}
Overriding design goals
   Extreme parallel scalability
      O(10^6)+ cores
   Extreme data structure scalability
      breadth: grid patches per AMR level
      depth: number of AMR levels
\end{verbatim}

\begin{verbatim}
Guiding goals
   scalable
   efficient
   correct
   agile
   complete
   resilient
   useable
   flexible
   modifyable
\end{verbatim}

\begin{verbatim}
 project scope
 arbitrary multi-level
 multi-physics
    nonlinear
    hyperbolic conservation
     hydrodynamics
    elliptic
     self-gravity
    parabolic?
    local physics
     chemistry
 user-supplied functions
    single-patch timestep advance
    inter-resolution constraints
    localized spacial resolution requirements: refinement criteria
    localized timestep requirements: CFL condition
 performance / efficiency of hardware utilization
    competetive serial performance
    competetive parallel performance
    highly competetive parallel scaling
    competetive memory usage
 data structures
    extreme particle count
    extreme grid patch count
    extreme hierarchy depth
 software resiliency
    resistent to continuous hardware and software failures
       memory failure / local compute node limits
       compute component (compute node, socket, core) failures
       network failures
       disk failures
       algorithm failure
\end{verbatim}

\begin{verbatim}
 hardware support
    multicore
    gpu / accelerators
    distributed and / or shared memory
 parallel technology support
    directly: CHARM++, MPI, UPC, OpenMP, + combinations
    indirectly: GPU / accelerator
 I/O
\end{verbatim}

\begin{verbatim}
 what we do not do
    visualization
    analysis utilities
\end{verbatim}

\begin{verbatim}
 capabilities
    physics
       hyperbolic
       elliptic
       ray-tracing?
    AMR
    Particles
    performance
    parallel scaling
    data structure scaling
    ease of use
    dynamic scheduling
    aggressive minimization of global communication
    I/O
    fault tolerance
    adaptivity
    extensibility
\end{verbatim}

\begin{verbatim}
Routines for data dumps, checkpointing, output of visualizaton /
  analysis
Leverage standard for AMR data output when possible
parallel HDF5
I/O performance
   design: support I/O from subset of compute nodes , dedicated or shared, to
           optimize I/O performance, and overlap I/O with computation
\end{verbatim}

\begin{verbatim}
automatic performance tuning
   computational block size, padding
   AMR patch size (by level)(possible?)
   refinement factor (2,4,8)
   load balancing frequency
\end{verbatim}

%=======================================================================
\section{Software Design} \label{s:design}
%=======================================================================


\begin{verbatim}
OOP
still primarily in design phase
   available flexibility to modify current design
   design as described is current snapshot
   little code and time wasted
emphasis on thorough designing before coding
   prototype ideas before final implementation
   more time upfront known to lead to faster overall development times
\end{verbatim}

\begin{verbatim}
application driven
   helpful for ensuring completeness
      missing functionality in design will become apparent
   helpful for testing
   Enzo II
   cosmology / astrophysics
   requires wide range of AMR capabilities
     broad for galaxy structure formation
     deep for star formation
     turbulence
   requires wide range of physics capabilities
     hyperbolic: hydrodynamics
     elliptic: self-gravity, FLD radiation
     local physics: chemistry, heating/cooling
   decouple physics modules from AMR framework
   make both Enzo II and underlying AMR framework publicly available
\end{verbatim}

\begin{verbatim}
relative coordinates for patches   
   relative to neighbors
   computations frequently only require cell size and timestep
      hydrodynamics
   absolute coordinates computed dynamically when necessary to required precision
      initialization
      inline analysis
      global position dependent physics (materials)
\end{verbatim}

\begin{verbatim}
local time-stepping
   global scaling of dx, dt may not be needed
      only power-of-two quantization of dx, dt
        mantissa unchanged--no fp precision issue
        fp range issue
            support coordinate rescaling at deep levels
\end{verbatim}

\begin{verbatim}
iterative development
   requirements, design, implementation, tests, user documentation updated in sequence
\end{verbatim}

\begin{verbatim}
Design goals
  extreme parallel scalability
  extreme data structure scalability
  breadth: number of grid patches
  depth: number of AMR hierarchy levels
\end{verbatim}

\begin{verbatim}
for parallel scalability: aggressive minimization of
  synchronization and collective operations
``localize'' problem as much as possible
  For each patch, only store patch, location information for
    neighboring patches (``ghost patches''), and ancestors for octree
  patch-local adaptive timestep control
    prevents rapidly evolving feature in one localized region of
      domain from determining timestep elsewhere
facilitate dynamic load balancing, dynamic scheduling of
  communication and computation
maximize data structure flexibility
dynamically optimize data structure parameters to hardware
  during run time
for algorithmic scalability: aggresively minimize dependencies
  of fp and integer precision on data structure size
related to ``localization'' of tasks
  particle positions stored using local coordinate system with
    origin at patch corner
  absolute patch coordinates not stored, only position relative
    to neighbors
     global coordinates only needed for initialization
     only need mesh width and timestep size for advancing
     not needed for restarts
     can still be computed dynamically
     localization an option--not needed for smaller problems
     absolute particle positions / grid locations may be needed
       for some output, e.g. global particle analysis
     in that case low precision
     still scaling issues relative to depth
      neighbor location
\end{verbatim}

%-----------------------------------------------------------------------
\subsection{Software components} \label{ss:components}
%-----------------------------------------------------------------------

\begin{verbatim}
Code organization: components based on functional decomposition
   High level components: Control
      Schedule, Distribute, Task
   Middle level components: Data and algorithms
      Simulation
         defines simulation
      Method [+ analysis / visualization]
         hooks to user code
         dynamic method application
            different physics for different resolutions
            different algorithms for increased software resilience
         multiple methods permitted
         selectable and scheduled at run-time
      Field / Particles 
      Amr / Patch / Tree / Node / Array : storing and accessing AMR
      Block / Fluxes: computing and communicating
   Low level components: hardware 
      Disk, Memory, Parallel      
   cross-cutting components
      Performance, Error, Monitor, Portal, Parameters
      globally-accesible functions
      components in aspect-oriented programming paradigm
         not sufficient for including specialized language (e.g. AspectC)
         techniques (e.g. templates) usable
\end{verbatim}

\begin{verbatim}
organize into components (subdirectories) of related classes
  Simulation, Control, Parallel, Task
  Amr, Patch: stores the AMR hierarchy
  Fields, Particles, Array: for storing data
  Block, Fluxes: for computing / communicating data
  Performance, Error, Parameters, Memory
    globally-accesible functions
    components in aspect-oriented programming paradigm
    not sufficiently important to use AspectC
    techniques usable
  Method [Analysis] [Visualize]
     hooks to user code
  External, User
\end{verbatim}

\FIGURE{Software component diagram}{f:components}{
\begin{minipage}{6.15in}
\includegraphics[width=6.15in]{components2.pdf}
\end{minipage}}

%-----------------------------------------------------------------------
\code{Arrays} Component
%-----------------------------------------------------------------------

\begin{verbatim}
Allow multiple root-level patches per MPI task. [To improve
  cache use for unigrid problems, and improve load-balancing for AMR.]
\end{verbatim}

%-----------------------------------------------------------------------
\code{Control} Component
%-----------------------------------------------------------------------

\begin{verbatim}
Reduce implicit dependencies by dynamically allocating parallel
  tasks, ala CHARM++.(e.g. currently Enzo loops through patches within
  a level, but a given patch can proceed as soon as it has all its
  boundary data)
Support optional uniform timesteps across all levels. [To
  improve parallel efficiency.]
Support optional variable timestep sizes within each level. [To
  reduce synchronization costs when computing global CFL condition.]
\end{verbatim}

%-----------------------------------------------------------------------
\code{Storage} Component
%-----------------------------------------------------------------------

\begin{verbatim}
Enforce strict control over data storage formats (e.g. files)
  (see W0009)
Require that all stored data be accessed through standard
  interface functions that are independent of specific file formats
  (i.e., stored datasets are conceptually treated as objects)
\end{verbatim}

%-----------------------------------------------------------------------
\code{Fields} Component
%-----------------------------------------------------------------------

\begin{verbatim}
User-controlled optional floor/ceiling limits on individual
  Fields (ala ``tiny\_number'' in Enzo), with user-specified Error
  behavior (warning, error, ignore, reset to given floor/ceiling,
  etc.)
\end{verbatim}

%-----------------------------------------------------------------------
\code{Methods} Component
%-----------------------------------------------------------------------

\begin{verbatim}
Integrate ``inits'' functionality into the main code
user functions supplied
   field / particle initialization
   particle advance
   particle computation
   particle - grid interaction
   block advance (multiple)
   inter-level flux correction
   refine / coarsen
   timestep determination
   constraint enforcement (div B)
   error checking / invariant checking for fault tolerance / software resiliency
      detects memory / cpu errors and mark core as faulty and bypass
   elliptic support:
      global and level operations / solves
\end{verbatim}

%-----------------------------------------------------------------------
\code{Parallelization} Component
%-----------------------------------------------------------------------

\begin{verbatim}
Load-balance by having over-loaded processors reassign tasks to
  random processes. [To reduce global communication for determining
  which processes are under-loaded]
Load-balance using ``over-compensation'', since heavily-loaded
  processes tend to continue to become more heavily loaded (cosmology
  / star-formation application-dependent).
Only use inter-core, inter-cpu, inter-node, etc.
  level-communicators to bound communicator size and manage
  communication nonuniformity.
Support multiple (hybrid) and flexible parallelization
  strategies, including MPI-1 (2-sided send/recv), MPI-2 (1-sided
  get/put), OMP, and optionally UPC and GPU.
hierarchical synchronization and collective operations
\end{verbatim}

%-----------------------------------------------------------------------
\code{Parameters} Component
%-----------------------------------------------------------------------

\begin{verbatim}
Support for user-supplied code for problem initialization.
\end{verbatim}

\code{Particles} Component

\begin{verbatim}
Store particle positions in single precision as -1 <=  x,y,z <= 1
  relative to their containing patch. [To reduce storage, improve
  performance, and address precision issues with deep AMR.]
Use a binary tree data-structure to recursively partition the
  bounding boxes of particles.
\end{verbatim}

\code{Simulations} Component

\begin{verbatim}
Support ensembles within a single run, including inline-analysis
\end{verbatim}

\code{Performance} Component

\begin{verbatim}
integrated performance monitoring
   summaries at different hardware levels
   less frequent at lower levels--more data
   more frequent at upper levels
   performance data available to other components
      load balancing based on actual memory usage / cpu time
      feedback for adaptivity
      help identify performance and scaling issues early
      poor-performance resilient
\end{verbatim}

\code{Portal} Component

\begin{verbatim}
portal: support interfacing with other codes
   post-processing solvers
   data analysis pipeline
   visualization pipeline
      use existing library, e.g. Visit
      Method can include visualization or inline analysis
\end{verbatim}

\code{Monitor} Component

\begin{verbatim}
monitor: support for interfacing with user while running
   ``dashboard'' for real-time monitoring state of simulation
\end{verbatim}


%-----------------------------------------------------------------------
\subsection{AMR data structures} \label{ss:datastructures}
%-----------------------------------------------------------------------

\begin{verbatim}
ALGORITHMS: and how problems solved
   AMR
   particles
   parallelization
   load balancing
   fault tolerance
\end{verbatim}

\begin{verbatim}
AMR basic unigrid
   unigrid is single AMR tree node(?)
     (how to link with level 1?)
\end{verbatim}

\begin{verbatim}
patch-local coordinates for particles
   control accumulation of roundoff
   minimize / eliminate effects of limited precision and range
   absolute coordinates computed when necessary
      analysis
      visualization
\end{verbatim}

\begin{verbatim}
octree AMR versus SAMR   
   octree advantages
      arguably more scalable

   octree limitations
      Morton ordering not always feasible
         different patch sizes
         different particle counts
         adaptive time-stepping on finer levels
         physics method variations
            shock capturing--Riemann solver iterations
            variable subcycling of iterative stiff methods
\end{verbatim}

\begin{verbatim}
AMR hierarchy
   generalized octree-based AMR 
     8-tree, sparse 64-tree, sparse 512-tree
     decoupled AMR / Array
   Fully distributed AMR data structure
   For each local grid patch, only immediate neighbors and
     ancestors stored
     Proxies for remotely stored patches
       remote thread identifier + pointer
   Store only actual data
     use flux registers for communication
     allocate patch ghost zones only for computation
   Could store appropriate subset of neighbor data as well for
     fault-tolerance
   AMR operations
     initial grid generation
     refinement criteria: tag refine or coarsen
     refinement or coarsening localized
\end{verbatim}

\begin{verbatim}
AMR hierarchy
   enhancements for both ``deep'' and ``shallow'' AMR

   Do not store a patch's global position, only local position
     relative to immediate neighbors, parent, and children. [Toward
     distributed AMR data-structure, and to address precision issues with
     deep AMR. Potential issues: boundary and initial conditions.] (see
     W013)
   For very deep AMR where coarser levels never complete their
     timestep, delete coarse levels to free storage.
    Provide (or notify) neighboring patches with updated
     ghost zone data as soon as it's available.
    Support temporary ``allocate as-needed'' ghost zones
     in addition to ``permanent'' ones
   Reduce tree-AMR node size by only storing parent, single
     neighbor, and single child. [Assuming one pointer for field data and
     SAMR patches indexed by single-precision offsets into parent
     patches, 32 bytes / tree-AMR and 48 bytes / patch-AMR]
   Relax rigid refinement criteria to inhibit excessive changes in
     octree-like tree refinement.
   Represent patch extents with (small) integer values relative to
     parent. [To reduce memory usage with deep AMR runs.] (see W012)
   Support both structured AMR (Enzo-like) and tree-based AMR
   Support flexible tree node types: memory-efficient versus
     compute-efficient.
\end{verbatim}

\begin{verbatim}
distributed AMR hierarchy
   local patches: store parent, neighbors, children (sparse)
   not as memory efficient as globally stored nodes 
      (3 pointers, 24 bytes average)
      cite (petascale AMR?)
   proxies for remotely stored patches
\end{verbatim}

\begin{verbatim}
relaxed local timestep control
   improves performance by not restricting timestep globally
   adaptive timesteps important for deep AMR runs, e.g. star formation (cite Abel)
   reduces global synchronizations for extreme scaling
   reduces global dependencies for extreme scaling
   allow variable timesteps within level
      eliminates requirement for global reduction
      timesteps quantized by two
         eliminates roundoff errors
      enforce timestep balancing--temporal level jumps(?)
\end{verbatim}

\begin{verbatim}
particles
   groups of related particles stored together
   particles associated with grid patch
   flexibility in particle ownership
     may belong to immediate neighbor to reduce communication
\end{verbatim}

\begin{verbatim}
AMR hierarchy data
     fields at multiple refinement levels 
       (internal tree nodes as well as leaves)
       SEE PARAMESH user guide for motivation
       improving solution
       refinement criteria (cite Berger)
     fields only finest level (only tree leaves)
       reduced memory usage
       reduced computation: only finest level
       reduced communication: no inter-level interpolation (except refinement)
\end{verbatim}

%--------------------------------------------------
\subsubsection{Patch coalescing } \label{sss:patch-coalescing}
%--------------------------------------------------

\begin{verbatim}
AMR algorithm enhancements: patch coalescing
   improves ``shallow'' octree-based AMR
   SAMR does this naturally--octree-based AMR doesn't
   especially helpful when large connected regions of domain at finest level
   less AMR overhead: fewer nodes / patches
   task size still controlable (fixed or restricted) through computational registers
       otherwise large patches an impedement to load balancing
   EXAMPLE: cosmology
\end{verbatim}

\begin{verbatim}
   variable patch resolution
   instead of uniformly refining a patch, can replace with higher
     resolution array
   need AMR machinery where resolution requirements change, not
     where high resolution is needed
   reduced AMR overhead (number of patches) especially in shallow
     AMR relative to PARAMESH
   improved performance since locally unifrom grid instead of AMR
     patches
   still supports variable block sizes for optimizing memory
     hierarchy, parallel task size, task counts
\end{verbatim}

\FIGURE{Coalescing}{f:coalesce}{
\begin{minipage}{3.75in}
\includegraphics[width=3.75in]{coalesce.pdf}
\end{minipage}}
\FIGURE{Targeted refinement with backfill}{f:backfill}{
\begin{minipage}{6.15in}
\includegraphics[width=6.15in]{kd-backfill.pdf}
\end{minipage}}


\FIGURE{
Coalesced patches proof-of-concept using a 2D cosmology density field projection.
\textbf{Left}: 2D cosmology density field projection.  
\textbf{Middle}: Balanced octree with 81701 patches.
\textbf{Right}: Balanced octree with 32529 coalesced patches. \\
}
{f:cosmo}{
\begin{minipage}{7.0in}
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{cosmo2-invert.png}
\end{minipage} \ 
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{cosmo2-4-1-inv.png}
\end{minipage} \ 
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{cosmo2-4-2-inv.png}
\end{minipage}
\end{minipage}
}

%--------------------------------------------------
\subsubsection{Targeted refinement} \label{sss:targeted-refinement}
%--------------------------------------------------

\begin{verbatim}
AMR algorithm enhancements: targeted refinement by 4 or 8
   improves ``deep'' AMR
   refinement by 2 too ``shallow''
   back filling possible to regain effect of balanced tree
   sparse storage format of child patches to reduce storage 
   balancing octrees can have global effect
     balancing refinementy by 4 or 8 probabalistically (provably?) local
     no explicit AMR patches required for backfill--can live on child nodes 
     EXAMPLE: point refinement
   targeted refinement by 4 or 8
     especially effective for deep AMR, e.g. star formation with
       30+ levels
     still supports effective resolution jumps of 2 using backfill
\end{verbatim}

\FIGURE{
Targeted refinement proof-of-concept using multiple point sources in 2D.
\textbf{Left}: 2D point sources.  
\textbf{Middle}: Balanced octree with 2137 patches.
\textbf{Right}: Balanced octree with 158 explicit patches.
}
{f:dots}{
\begin{minipage}{7.0in}
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{dots-invert.png}
\end{minipage} \ 
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{dots-4-1-inv.png}
\end{minipage} \ 
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{dots-16-5-inv.png}
\end{minipage}
\end{minipage}}

%-----------------------------------------------------------------------
\subsection{Parallelization approach}
%-----------------------------------------------------------------------

\begin{verbatim}
hybrid parallel
   MPI + OMP
   MPI + UPC
   UPC + OMP (?)
   CHARM++ + OMP (?)
   flexible subset of cores, sockets, nodes, supernodes
   Task scheduling CHARM++ model, but implemented in MPI, UPC, OMP
   processor-task affinity
\end{verbatim}

%-----------------------------------------------------------------------
\subsection{Data distribution} \label{ss:data-placement}
%-----------------------------------------------------------------------

\begin{verbatim}
memory storage formats and data movement
   different formats for different uses
        optimized for storage
          option to not store guard / ghost cells
          AMR patches may be larger than computationally optimal
        optimized for computing
          store ghost cells
          improved data locality
          compute block size optimized for cache
             copied to array compute registers
             blocking/padding for cache / memory hierarchy
             groups of blocks for GPU / fp accelerators
   compute in reformatted ``computational registers''
      basic idea used in PARAMESH
         encapsulates functionality of both PARAMESH ``working block'' and ``work'' datastructures
      more flexible:
        arbitrary reordering of array axes
        arbitrary subset of axes
        arbitrary ghost zone depth
          more adaptable to existing legacy ``unigrid'' routines
          different reorderings possible for different methods (hydro, chemistry, etc.)
          flexibilty allows improved data-layout for single-thread performance
             cache blocking, padding
   communicate using reformatted ``flux registers''
      idea used in \code{Chombo}
   scheduler reuse for computation / communication scheduling
\end{verbatim}

%-----------------------------------------------------------------------
\subsection{Task scheduling} \label{ss:scheduling}
%-----------------------------------------------------------------------
\note{task scheduling summary}


\begin{verbatim}
dynamic scheduling
   task = grid patch + sequence of 
   dynamically schedule local tasks
      increased scheduling freedom
      improves slack for latency hiding
   schedule communication to/from local patches
   tasks scheduled biased towards patches that limit other patches, e.g. coarse
   optionally pipeline multiple methods on cores
      increase available parallelism: (e.g. deep AMR with many physics modules)
      helpful when multiple physics components available with similar costs
   communication scheduling to maximize releasing patches and prefetch data
   gang-scheduling groups of patches for improved GPU throughput / accelerator utilization
hierarchical / multiscale task scheduling
   multiblock (GPU)
   single block (process)
   plane / line (thread)
\end{verbatim}

Given that tasks are acceptably load-balanced across process, the
tasks must be schedule for execution.  The @@@


We will use dynamic scheduling of tasks assigned to a processes, since
it @@@  Dynamic scheduling is more suitable than static scheduling for
heterogeneous sized tasks, non-regular communication dependencies @@@

\note{task scheduling details}

After tasks are distributed among processing elements, the next
decision is how to schedule tasks among processing elements.  Two main
approaches are static and dynamic.  Static scheduling is preferred
when workload is evenly mapped to processing components, and when
there are relatively few tasks per process.  We will use dynamic
scheduling, since workload (communication as well as computation) per
task varies between tasks.

\note{CHARM++ summary} 
Our approach will be well-mapped to CHARM++,
which is a fault-tolerant message-driven parallel programming
framework.  CHARM++ also provides dynamic load balancing through task
migration, and provides fault tolerance through its built-in disk (or
memory + disk) checkpoint / restart mechanism.

\note{implementation in CHARM++}
Implementation of AMR for a hyperbolic problem, a parallel task, or
CHARM++ ``chare'', would correspond to advancing a numerical method or
sequence of methods one timestep on a single grid patch block.  The
exchange of guard cell data would correspond to CHARM++ messages.
This implementation would allow a block to advance one timestep as
soon as all of its guard cell data is available.

\note{non-CHARM++ implementation}

We also plan to develop our own scheduler, for several reasons.  One,
to schedule communication in addition to computation, to help hide
latency through prefetching.  Second, to take advantage of
hierarchical hardware components (node, socket, core...)  Third, to
remove any hard dependencies on any single parallel technology.

%-----------------------------------------------------------------------
\subsection{Load balancing} \label{ss:loadbalancing}
%-----------------------------------------------------------------------

Dynamic load balancing is well known to be a crucial operation for
extreme scalability in general, and for extreme adaptive mesh
refinement in particular.  Computational load must be evenly
distributed to maintain high overall parallel computational
efficiency; dynamically allocated memory must be well-distributed
across compute nodes to avoid running out of physical memory; and data
locality must be maintained within compute nodes to maintain high
communication performance over the interconnect.

A highly scalable approach of rebalancing octree-based SAMR
hierarchies is to linearize the data structure using a Morton or
Hilbert type space-filling curve, then partition the tree nodes among
processes by dividing the linearized structure into $N_P$ evenly sized
pieces.  This approach has been shown to scale to over $60K$
cores~\cite{@@@ART}.  It works well if workload and memory usage are
proportional to each other within tree nodes, and when workload
between tree nodes is roughly equal; however, neither of these
conditions is met when adaptive time-stepping is used.  Also, this
approach still requires $O(N_P)$ storage, and a global all-gather
operation (e.g.~\code{MPI\_Allgather}) is required on all processes.

@@@@@@@@@@@@@@@@@@@@

Our approach will be to 

\begin{verbatim}
 load balancing
    hierarchical
       improved scalability
       improved flexibility
       improved efficiency
    load balance locally in given level
    different metrics at different levels
      memory at node level
      workload below
    different frequency at different levels
      lower frequency balancing at higher levels
         more costly
         solution changes less frequently
      higher frequency balancing at lower levels
         less costly
         solution changes more frequently
    task adjacency maintained
    use collected performance data
    explore overcompensation technique
       ala SOR
       two regimes
          local collapsing / explosion
          shock advancement
       identify regimes and adapt
    linearized morton curve / hilbert curve insufficient
       assumes equal work per patch
       particles are associated with nodes, changing weight
       adaptive time-stepping drastically weights more highly  refined patches
       performance of physics algorithms on a patch is not necessarily uniform
          localized chemistry subcycling, front tracking, etc.
       arrays on patches may be different sized (assuming coalesced patches)
       linearization constricts dimensionality
          constrains data movement along a single dimension
          physics imbalances 4 dimensional
\end{verbatim}

\begin{verbatim}
task migration
   migrate tasks dynamically
   CHARM++ model: pack, relocate, unpack data
   maintain locality by moving task to owner of a neighbor
   maintain parent-child locality when possible
      relax for deep hierarchies
   migrate using hierarchical parallelism
\end{verbatim}

(e.g.~load balancing
across $\approx 2^20$ cores can be replaced by four hierarchical
load balancing steps across $\approx 2^5$)


for each node, and if the global \code{MPI\_AllGather} of long
integers on $N_C$ cores is acceptable \footnote{\cite{BuGh08}
  indicates $N_C$ cores, though we believe with a hybrid programming
  approach their global reduction could be reduced to an even more
  scalable $N_P$ MPI processes}.  However, we anticipate that this
approach will not be sufficient for a general-purpose extreme AMR
framework for several reasons.
%
One, adaptive time-stepping is required for efficient ``deep'' AMR
problems, but that scales the work load for a given node by a
non-constant factor of roughly $2^k$ for level $k$.
%
Two, particle distributions among nodes may not be uniform, affecting both memory and computational loads.
%
Three, array patches may be differently sized, if we use our ``coalesced patches'' enhancement to reduce the AMR tree node count.  
%
Four, performance of physics methods on a patch is not necessarily
uniform, due to, e.g., localized subcycling of stiff methods, front
tracking methods, etc.  The first three issues, and possibly the
fourth, could be addressed by dynamically weighting nodes before
partitioning them among processes, but that would require additional
global communication to perform the reduction.
%

Lastly, we suspect that linearizing the workload artificially
restricts the flexibility of data movement allowed (along one
dimension instead of three), creating more communication traffic
during task migration.  than in not be optimal in terms of the amount
of data that needs to be redistributed during each rebalancing step,
since it restricts workload along a single dimension, whereas the
actual workload is typically distributed along three or four
dimensions (three spacial and one time).

Since we suspect that a space-filling curve approach is only effective
for a strict subset of AMR problems we wish to support, we will
explore two ideas for dynamic load balancing, including ``hierarchical
load balancing'', and a new approach we call ``deliberate
overcompensation''.

\note{load-balancing: hierarchical} 
%
By \textit{hierarchical load balancing} we mean rebalancing tasks
between higher levels (nodes or supernodes) independently from
rebalancing between lower levels (socket or cores).  The idea of
hierarchical task scheduling and load balancing has been known for at
least 15 years (\cite{AhGh94}, but is not commonly used for AMR
frameworks.

This will have several advantages over more traditional
non-hierarchical load balancing.  First, rebalancing at different
levels could be based on different metrics, e.g. memory usage for
rebalancing between nodes, and computational load for rebalancing
between sockets.  This will be particularly useful for AMR problems,
because memory is the crucial metric we want to keep balanced at the
node level, yet we want to balance computational time at lower
hardware platform levels to keep computational elements busy.  Second,
rebalancing frequencies at different levels can be decoupled,
rebalancing less frequently at the upper node levels where rebalancing
is more costly, and where load tends to take longer to become
unbalanced.

\note{load-balancing: deliberate overcompensation}
%
By \textit{deliberate overcompensation} we mean rebalancing by
relocating \textit{more} than enough tasks from high-load to low-load
processes.  The motivation is for physics phenomena, such as
gravitational collapse or explosions, that require a continuous and
regular redistribution of computational resources.  The idea is
conceptually analagous to the successive over-relaxation (SOR) variant
of the Gauss-Seidel method.  This approach could potentially reduce
the frequency of rebalancing the load by half with the same tolerance
on imbalance.

%
\note{load-balancing: data locality}
Data locality is crucial for parallel performance, so tasks associated
with neighboring patches and parent-child patches are kept on the same
process when possible, or failing that, on ``close'' processes (same
socket or node) if possible.  This will be maintained by weighting
migration of tasks to closer processes (through hierarchical load
balancing), and migrating a task to another process only if a
neighbor, parent, or child of the task is associated with that
process.  This will enforce the spacial data associated with tasks
assigned to a process to be simply connected.  We also wish to keep
the surface area-to-volume ratio low.  The space-filling curve
approach is a specialization of maintaining simply connectednes, and
we expect the additional generalization can be used to improve data
locality, surface-to-volume ratio, and reduce task migration traffic.

``diffusion-based scheme''

%-----------------------------------------------------------------------
\subsection{Software resilience} \label{ss:resilience}
%-----------------------------------------------------------------------

\begin{verbatim}
software resilience
   take advantage of only Methods change data
   methods signal which fields / particles changed
\end{verbatim}
\begin{verbatim}
fault-tolerance / software resilience strategies
   need to deal with continuous stream of failures
   MTTF < MTTC
   checkpoint to disk
      issue: failures will become more frequent than time to checkpoint
      agressively reduce checkpoint data size and write time
         dedicated I/O nodes
         compress
         check data
         methods identify which data modified
            may help lower disk output--only checkpoint modified data
   checkpoint to memory
     CHARM++ does this(?)
   detect hardware errors and mark as defective
       memory
       disk
       core
       socket
       node
       interconnect (pairs of nodes)
       software libraries (MPI versus UPC, etc.)
   flash memory
   log faults to disk for subsequent analysis
   performance resilience
      dynamically adapt to reduce cache thrashing / ineffeciency
          array blocking or padding in computational array registers
      adapt AMR patch size, refinement factor (2,4,8)
   fault-tolerant MPI
      FT-MPI
   leverage new approaches when available
      active research area
      keep up to date in latest practices
      design software to use new approaches
\end{verbatim}

%-----------------------------------------------------------------------
\subsection{I/O} \label{ss:io}
%-----------------------------------------------------------------------

\begin{verbatim}
I/O different output formats for different uses
   checkpointing
   analysis
   visualization
   general ``data dump''
   cheaper to rerun and regenerate data to process than dump and reread later
      inline analysis capability to reduce overall output required
   checkpoints
       node / processor independent: software resiliency
       checkpoints restartable on different configurations / platforms
   ``accessor code'' included with all output data
\end{verbatim}
\begin{verbatim}
I/O
   parallel HDF5
   compression
   CRC error-checking and retry
   subset of nodes do I/O
   detection of faulty disk and mark as unusable
   different formats for different uses
\end{verbatim}

%=======================================================================
\section{Implementation} \label{s:implementation}
%=======================================================================

\begin{verbatim}
Trac + SVN or mercurial
documenting as I go
OOP design: improves component reuse, controls software
  complexity, eases software maintenance
heavy emphasis on designing before coding
\end{verbatim}

\begin{verbatim}
interdependencies controlled at component and class level
classes with access functions used instead of raw arrays to improve modularity and modifiability
block structure for interface with user code
unit testing, including performance and parallel scaling
prototyping for proof-of-concept
application testing
code reviews
\end{verbatim}

\begin{verbatim}
Languages C++; user code can be C, C++, Fortran
attempt to componentize parallelization : MPI (two-sided, one-sided) + OMP + UPC + GPU + CHARM++
advantages / disadvantages of each
\end{verbatim}

\begin{verbatim}
Multiple parallelization strategies
  MPI: + widespread, optimized implementations, familiar
  MPI: - data replication, difficult to use
  UPC + easier to use, combines shared memory view with efficient data affinity
  UPC - no concept of MPI communicator, still under development--not as mature
  OMP + can be used progressively
  OMP - not scalable outside of socket / node;  inefficiencies due to false cache sharing
  GPU + very fast / power efficient when usable
  GPU - no usable standard, difficult to program, difficult to map problem to hardware
  CHARM++ + higher-level, dynamic scheduling, dynamic load-balancing, fault tolerant through checkpointing to other node memory
  CHARM++ - requires learning separate language, separate runtime system, no data prefetching(?)
    currently not fully realizable for GPU since depends on
      computational code
    hierarchical parallelism: MPI + OMP, MPI + UPC, MPI + GPU,
      etc.
    advantages of hybrid
      reduced data replication from MPI distributed memory
      dynamic parallel threads--use more when helpful, fewer when not
      UPC
    disadvantages of hybrid
      performance hit from data sharing in MPI + OMP
      MPI and UPC communication cannot (currently) proceed concurrently
    code for two modes: distributed memory and shared memory
    parallel tasks: grid patches, arrays, grid patch groups,
      particle groups
    flexible data structure parameters (grid patch size, patch
      decomposition, patch grouping) to dynamically optimize task size
\end{verbatim}

\begin{verbatim}
  TRAC
    wiki used for design
    tickets for tasks and documenting defects
  Subversion
\end{verbatim}


\begin{verbatim}
  Languages
     C++, C99, legacy Fortran
  Libraries: HDF5
\end{verbatim}

\begin{verbatim}
  Languages: C++, C99, UPC, OpenMP
  Parallel frameworks / languages / libraries / directives
     CHARM++
     MPI (one-sided and send-recv)
     UPC
     OpenMP
  CUDA / OpenCL (user code only)
\end{verbatim}

\begin{verbatim}
  documentation
\end{verbatim}

\begin{verbatim}
  quality control
    testing 
       unit testing
       regression testing
          lcatest parallel testing framework
          correctness
          performance
             built-in performance monitoring
             single-thread
             weak and strong parallel scaling 
             communication performance
             I/O performance
          data structure scaling
             extreme broad problems (grid / particle counts)
             extreme depth problems
          software resilience / fault tolerance
             memory failures
                fill
                load balancing for memory
             compute failures
                tag component (cabinet, node, socket, core) as unusable
             network failures
                checksums
                reroute P1->P2 as P1->Pj->P2
             disk failures
                checksums
             algorithm failure
                support adaptive algorithms
                locally override spacial mesh width / timesteps
       software reviews
          testing + reviews highly effective [CITE code complete]
   refactoring
      iterative development: requirements - design - implementation - testing   
development
   implement progessively to fill user beta testing pipeline
\end{verbatim}

\begin{verbatim}
testing
   lcatest: automated parallel application testing
   multiple test levels
      unit tests
      component tests
      application tests
      in-house / community beta-testing
         (progressive as functionality comes online)
   test for multiple things:
      functionality
      correctness
      performance
      scaling
   tests also help supplement user documentation
   use integrated performance monitoring
      PAPI for hardware counters
      PMPI for MPI communication
      new[] / delete[] overload for dynamic memory usage
         particularly important for AMR
      user-defined independent attributes
         cycle
         level
         process
      user-defined dependent metrics
         process-local patch counts
         process-local cell / zone counts
         process-local particle counts
      less frequent output at finer levels (more data)
   helps identify functional, performance, scaling bugs early
   extreme scaling designed into framework from the start
\end{verbatim}

%-----------------------------------------------------------------------  
\subsection{Parallelism} \label{ss:parallelism}
%-----------------------------------------------------------------------  

\begin{verbatim}
CHARM++ 
  data placement
  load balancing
  task scheduling
  checkpointing for fault tolerance
  performance monitoring and visualization
\end{verbatim}

\begin{verbatim}
platform hierarchical architecture-aware data structures
  e.g. MPI communicator for cores in a socket, sockets in a
    node, nodes in a supernode, supernodes in a machine
  facilitates hierarchical dynamic load balancing
    improves dynamic mapping of data structures to hardware components
    E.g. load balance more frequently at core / socket level to
      keep functional units busy
    load balance node / supernode levels less frequently to keep
      memory usage uniform
    less frequent because:
      problem changes less at larger scales
      rebalancing is more expensive: larger data sizes, slower
        interconnects
    user-defined parameters and metrics for load balancing at
      different levels
      dynamically collected performance data can be fed back
        into hierarchical load balancing algorithm
  note linearization of octree datastructure is insufficient:
    assumes equal work per patch
    particles are associated with nodes, changing weight
    adaptive time-stepping drastically weights more highly
      refined patches
    performance of physics algorithms on a patch is not
      necessarily uniform, e.g. localized chemistry subcycling, front
      tracking, etc.
    arrays on patches may be different sized
    linearization of patches 
        reduces flexibility
        constrains data movement along a single dimension
\end{verbatim}

\begin{verbatim}
hierarchical parallelism
   encourage communication within hardware components
      sockets within node
      cores within socket
      hyperthreads within core
\end{verbatim}

\begin{verbatim}
parallel technology encapsulation / processor virtualization
  distributed / shared memory
    MPI (two-sided and one-sided) (distributed memory)
    OpenMP (shared memory) 
    UPC (either distributed memory or shared memory)
    CHARM++
  multiple strategies enhance software resiliency
    i.e. buggy MPI implementation--dynamically switch to UPC
\end{verbatim}

%-----------------------------------------------------------------------  
\subsection{Fault tolerance} \label{ss:fault-tolerance}
%-----------------------------------------------------------------------  

\ \\ \pargraph{fault tolerance} Fault tolerance and software
resilience are crucial factors at extreme scales, since it has been
observed that frequency of failures is proportional to the number of
sockets~\cite{@@@}

\begin{verbatim}
 FT-MPI 
  ``fault-tolerante MPI''
  http://icl.cs.utk.edu/ftmpi/overview/index.html 
MPICH-V 
  ``MPI Implementation for Volatile resources''
  http://mpich-v.lri.fr/index.php 
\end{verbatim}

%=======================================================================
\section{Development Plan} \label{s:plan} 
%=======================================================================

\begin{verbatim}
  CHARM++ generalized unigrid
     evaluate effectiveness
  AMR hyperbolic
  particle methods
  AMR elliptic
\end{verbatim}

%=======================================================================
\section{Milestones and Deliverables} \label{s:milestones}
%=======================================================================

\begin{verbatim}
   Cello software framework
   research papers
   Complete Enzo II application
   large-scale demonstration using Enzo II
   workshop/training
\end{verbatim}

%=======================================================================
\bibliography{papers}
\bibliographystyle{unsrt}
%=======================================================================

\end{document}

%==================================================================

